# Firefly-master

## 项目简介

Firefly-master是一个基于Python的深度学习训练框架，主要用于训练和优化聊天机器人模型。

## 主要功能

- 提供预训练模型的训练和微调
- 支持多种训练模式，包括常规训练和LoRA训练
- 提供详细的训练参数配置
- 支持TensorBoard进行训练过程的可视化
- 提供模型保存和加载功能
## 训练模式
Firefly-master项目支持三种训练模式，如在`train.py`文件中所示：

- 完全训练模式（`full`）：这是标准的训练模式，其中所有模型的参数都会被训练。

- LoRA训练模式（`lora`）：这是一种专门的训练模式，使用LoRA（低秩适应）方法。在这种模式下，只有一小部分参数（权重矩阵的低秩分解）会被训练。

- QLoRA训练模式（`qlora`）：这是LoRA训练模式的一个变体，使用量化以获得额外的内存和计算效率。

这些模式可以在训练参数的JSON文件中的`train_mode`字段中指定，例如`llama3-8b-sft-lora.json`。

## 优化方法
代码提供了三种优化方法(以下内容来自chatgpt)

- DPO (Direct Preference Optimization) 
  - DPO是一种优化方法，它将奖励最大化问题视为人类偏好数据上的分类问题。这种方法稳定、高效且计算量小，不需要奖励模型拟合、大量采样和超参数调整。DPO的工作流程包括监督微调（SFT）作为初始步骤，然后在SFT之后，模型使用偏好数据进行偏好学习。DPO的优点在于它的简单性，直接将偏好损失定义为策略的函数，无需首先训练奖励模型。在微调阶段，DPO使用大型语言模型（LLM）作为奖励模型，通过二元交叉熵目标优化策略，利用人类偏好数据来确定哪些响应是优选的。

- Pretrain (预训练) 
  - 预训练阶段是大型语言模型（LLM）训练过程的一部分，在这个阶段，模型在大规模通用语料库上进行训练，以学习语言的通用模式和结构。这个过程不针对特定的语言任务，而是让模型学习如何预测句子中的单词，这是学习语言模型的全部内容。预训练通常需要大量的数据（几十亿文字）和计算资源，可能需要几天到几个月的时间才能完成。

- SFT (Supervised Fine-Tuning) 
  - 有监督微调（SFT）是在预训练之后的一个步骤，这个阶段模型在特定任务的数据集上进行进一步的训练，以提高其在该任务上的性能。SFT侧重于有监督的学习，通常需要较少的数据（约10万个单词）。在SFT中，模型的一些层可能会被冻结，而其他层则进行调整以适应特定任务。SFT是一个重要的步骤，因为它可以让模型成为一个能够对用户提示作出类似人类回应的助手。
## 使用方法

1. 配置数据集，如data目录下示例文件
2. 运行train.py 微调模型
3. 运行script/chat/chat.py进行对话，支持接口访问
